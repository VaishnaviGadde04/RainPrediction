{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Date: 18/10/2019\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "\n",
    "* [Matplotlib Official Documentation)](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "        pip install matplotlib\n",
    "        \n",
    " \n",
    "* [OS Official Documentation)](https://docs.python.org/3/library/os.html)\n",
    "        pip install os\n",
    "\n",
    "\n",
    "\n",
    "# Rain in Australia: Predict rain tomorrow in Australia\n",
    "\n",
    "## Introduction\n",
    "This assignment comprises of two parts. In this part, you will analyze Predicting rain or weather is a common problem in machine learning. Different machine earning algorithms can be used to model and predict rainfall. In this assignment, we ask you to complete the analysis to predict whether there will be rain tomorrow or not. In particular, you are required to apply the tools of machine learning to visualize and predict the possibility of rainfall in Australia.\n",
    "\n",
    "Following is the given dataset:\n",
    "* [Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package)\n",
    "\n",
    "Following are the steps to be performed for Part A:\n",
    "1. [__Step 01__](#Step_01): In this step We will use and import **`SparkContext`** from **`pyspark`**, which is the main entry point for Spark Core functionality. The **`SparkSession`** object provides methods used to create DataFrames from various input sources.\n",
    "2. [__Step 02__](#Step_02): In this step I need to create a dataframe and give the source of input dataset.\n",
    "3. [__Step 03__](#Step_03): In this step I need to drop few columns which are not essential in the dataset.\n",
    "4. [__Step 04__](#Step_04): In this step I need to print number of missing values in the dataset. \n",
    "5. [__Step 05__](#Step_05): In this step I have to fill the missing values with average value for numeric columns & maximum occurence for categorical columns.\n",
    "6. [__Step 06__](#Step_06): In this step I need to perform Data transformation i.e., changing the datatype from string to double for numeric columns & using String Indexer method to convert into numbers for categorical columns.\n",
    "7. [__Step 07__](#Step_07): In this step I need to create a feature vector & randomly splitting the data\n",
    "8. [__Step 08__](#Step_08): In this step I had applied different machine learning classification algorithms on the dataset & comparing their accuracies.\n",
    "8. [__Step 09__](#Step_09): In this step I had calculated Confusion Matrix, Recall, Precision & F1 Score for different machine learning classification algorithms.\n",
    "\n",
    "More details for each steps will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Creating Spark Session and Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01: Import  Spark Session and initialize Spark <a id='Step_01' ></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext and SparkSession\n",
    "Apache Spark community released a powerful Python package, **`pyspark`**. Using **`pyspark`**, we can  initialise Spark, create RDD  from the data, sort, filter and sample the data. Especially, we will use and import **`SparkContext`** from **`pyspark`**, which is the main entry point for Spark Core functionality. The **`SparkSession`** object provides methods used to create DataFrames from various input sources.  \n",
    "\n",
    "Spark applications run as independent sets of processes on a cluster, which is specified by the **`SparkContext`** object. **`SparkContext`** can connect to several types of cluster managers (local (standalone), Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (passed to `SparkContext`) to the executors. Finally, **`SparkContext`** sends tasks to the executors to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code to create a sparkSession object, with 4 local cores. To create a sparkSession with 4 core you have to use configure it as `local[4]`. Given a name to your program using `setAppName()` as `Assignment 2_Rainfall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-27-160-193:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Assignment 2_Rainfall</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=Assignment 2_Rainfall>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "# create entry points to spark\n",
    "from pyspark import SparkContext,SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "# local[4]: run Spark locally with 4 working processors as logical cores on your machine.\n",
    "# The `appName` field iis set as `Assignment2_Rainfall`. \n",
    "conf = SparkConf().setAppName(\"Assignment 2_Rainfall\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sparkContext=sc)\\\n",
    "        .builder\\\n",
    "        .appName(\"MongoDB and Apache Spark\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\")\\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/fit5202_db.wk04_coll\")\\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/fit5202_db.wk04_coll\")\\\n",
    "        .getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 02: Load the dataset and print the schema and total number of entries<a id='Step_02' ></a>\n",
    "\n",
    "Firstly, I had read the csv file using `spark.read.csv` and stored it in a dataframe. Then printed the number of entries in the given dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the given dataframe: 142193\n"
     ]
    }
   ],
   "source": [
    "# Reading data from input csv file & displaying number of entries in the dataframe\n",
    "weather_df = spark.read.csv(\"weatherAUS.csv\", inferSchema=True, header=True)\n",
    "print(\"Number of entries in the given dataframe:\",weather_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the total number of entries, I had viewed the datatype of each column by using printschema & also seen the few contents in the given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema() # Printed schema for the given dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|               Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow|\n",
      "+-------------------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|2008-12-01 00:00:00|  Albury|   13.4|   22.9|     0.6|         NA|      NA|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       8|      NA|   16.9|   21.8|       No|          No|\n",
      "|2008-12-02 00:00:00|  Albury|    7.4|   25.1|       0|         NA|      NA|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|      NA|      NA|   17.2|   24.3|       No|          No|\n",
      "|2008-12-03 00:00:00|  Albury|   12.9|   25.7|       0|         NA|      NA|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|      NA|       2|     21|   23.2|       No|          No|\n",
      "|2008-12-04 00:00:00|  Albury|    9.2|     28|       0|         NA|      NA|         NE|           24|        SE|         E|          11|           9|         45|         16|     1017.6|     1012.8|      NA|      NA|   18.1|   26.5|       No|          No|\n",
      "|2008-12-05 00:00:00|  Albury|   17.5|   32.3|       1|         NA|      NA|          W|           41|       ENE|        NW|           7|          20|         82|         33|     1010.8|       1006|       7|       8|   17.8|   29.7|       No|          No|\n",
      "+-------------------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.show(5) # Printing the contents of weather dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Cleaning and Processing\n",
    "Data cleaning and processing is an important aspect for any machine learning task. We have to carefully look into the data and based on the types, quality of the data, we have to plan our cleaning procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 03: Delete columns from the dataset<a id='Step_03' ></a>\n",
    "\n",
    "During the data cleaning and processing phase, we delete unnecessary data from\n",
    "the dataset to improve the efficiency and accuracy of our model. You have to think\n",
    "which columns are not contributing to the rain prediction. To keep things simple, you are\n",
    "required to delete the following columns due to data quality and accuracy.\n",
    "\n",
    "● Date\n",
    "● Location\n",
    "● Evaporation\n",
    "● Sunshine\n",
    "● Cloud9am\n",
    "● Cloud3pm\n",
    "● Temp9am\n",
    "● Temp3pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `drop` command, I had dropped few columns from a dataframe.\n",
    "weather_df = weather_df.drop('Date','Location','Evaporation','Sunshine','Cloud9am','Cloud3pm','Temp9am','Temp3pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|   13.4|   22.9|     0.6|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       No|          No|\n",
      "|    7.4|   25.1|       0|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|       No|          No|\n",
      "|   12.9|   25.7|       0|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|       No|          No|\n",
      "|    9.2|     28|       0|         NE|           24|        SE|         E|          11|           9|         45|         16|     1017.6|     1012.8|       No|          No|\n",
      "|   17.5|   32.3|       1|          W|           41|       ENE|        NW|           7|          20|         82|         33|     1010.8|       1006|       No|          No|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the dataframe after dropping the above columns\n",
    "weather_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 04: Print the number of missing data in each column<a id='Step_04' ></a>\n",
    "\n",
    "Using a for loop I had calculated the count of NA(null) values in each column and then printed the number of NA(null) values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA(null) values in MinTemp : 637\n",
      "Number of NA(null) values in MaxTemp : 322\n",
      "Number of NA(null) values in Rainfall : 1406\n",
      "Number of NA(null) values in WindGustDir : 9330\n",
      "Number of NA(null) values in WindGustSpeed : 9270\n",
      "Number of NA(null) values in WindDir9am : 10013\n",
      "Number of NA(null) values in WindDir3pm : 3778\n",
      "Number of NA(null) values in WindSpeed9am : 1348\n",
      "Number of NA(null) values in WindSpeed3pm : 2630\n",
      "Number of NA(null) values in Humidity9am : 1774\n",
      "Number of NA(null) values in Humidity3pm : 3610\n",
      "Number of NA(null) values in Pressure9am : 14014\n",
      "Number of NA(null) values in Pressure3pm : 13981\n",
      "Number of NA(null) values in RainToday : 1406\n",
      "Number of NA(null) values in RainTomorrow : 0\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of NA(null) values in each column\n",
    "for column_name in weather_df.columns:\n",
    "    print(\"Number of NA(null) values in\",column_name,\":\",weather_df.filter(weather_df[column_name] == \"NA\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 05: Fill the missing data with average value and maximum occurrence value<a id='Step_05' ></a>\n",
    "\n",
    "- In this step you have to fill in all the missing data with average value (for numeric column) or maximum frequency value (for non-numeric column).\n",
    "- Firstly, identify the columns which have numeric values (e.g., MinTemp, MaxTemp), calculate the average and fill the null value with the average.\n",
    "- Secondly, identify the columns with non-numeric values (e.g., WindGustDir, WindDir9am) and find the most frequent item (e.g., wind direction). Now fill the null values with that item for that particular column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a for loop for reading all columns, and with the help of aggregate function calculated the `average` value for each column and store it in Average attribute. Average values are calculated for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Mean for numerical columns\n",
    "Average = {}\n",
    "for column in weather_df.columns:\n",
    "    value = weather_df.agg({column:'avg'}).collect()[0][0]\n",
    "    if value != None:\n",
    "        Average[column] = value\n",
    "    else:\n",
    "        Average[column] = weather_df.agg({column:'max'}).collect()[0][0]\n",
    "Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using groupBy & aggregate function, I had counted the maximum occurence of string & using Orderby I had calculated the count for Categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing NA value with Max occurence value for categorical columns\n",
    "Average['WindGustDir'] = weather_df.groupBy('WindGustDir').agg({'WindGustDir':'count'}).orderBy('count(WindGustDir)').collect()[16][0]\n",
    "Average['WindDir9am'] = weather_df.groupBy('WindDir9am').agg({'WindDir9am':'count'}).orderBy('count(WindDir9am)').collect()[16][0]\n",
    "Average['WindDir3pm'] = weather_df.groupBy('WindDir3pm').agg({'WindDir3pm':'count'}).orderBy('count(WindDir3pm)').collect()[16][0]\n",
    "Average['RainToday'] = weather_df.groupBy('RainToday').agg({'RainToday':'count'}).orderBy('count(RainToday)').collect()[2][0]\n",
    "Average['RainTomorrow'] = weather_df.groupBy('RainTomorrow').agg({'RainTomorrow':'count'}).orderBy('count(RainTomorrow)').collect()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the NA values of numerical columns with its Average values\n",
    "from pyspark.sql.functions import when\n",
    "for column in weather_df.columns:\n",
    "    weather_df = weather_df.withColumn(column, when(weather_df[column] == \"NA\", Average[column]).otherwise(weather_df[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the dataframe after imputation\n",
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 06: Data transformation<a id='Step_06' ></a>\n",
    "\n",
    " - Before transforming your non-numerical data, do the type casting (to double) of the numerical value columns as they are defined as “String” (see, the schema of the dataset). \n",
    " - For the non-numerical value column (i.e., WindGustDir,\n",
    "WindDir9am, WindDir3pm, RainTomorrow) use the StringIndexer method to convert them into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only the numeric columns in a separate list\n",
    "numeric=[\"MinTemp\",\"MaxTemp\",\"Rainfall\",\"WindGustSpeed\",\"WindSpeed9am\",\"WindSpeed3pm\",\"Humidity9am\",\n",
    "                  \"Humidity3pm\",\"Pressure9am\",\"Pressure3pm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type casting numerical columns from string to double type\n",
    "for column in numeric:\n",
    "    weather_df=weather_df.withColumn(column,weather_df[column].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the datatype of columns after typecasting with the usage of `printSchema`\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing String Indexer for changing the datatype of categorical column\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only the categorical columns in a separate list\n",
    "categorical=['WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Stringindexer casting the datatype of categorical column value with Pipelining\n",
    "l_indexer = [StringIndexer(inputCol=column, outputCol=column+\"labelIndex\").fit(weather_df) for column in categorical]\n",
    "# Convert label from string to index\n",
    "pipeline = Pipeline(stages=l_indexer)\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineModel = pipeline.fit(weather_df)\n",
    "# Creating a model to transform the given dataframe to new one\n",
    "weather_df2 = pipelineModel.transform(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `drop` command, I had dropped original columns from a dataframe.\n",
    "weather_df2 = weather_df2.drop('WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 07: Create the feature vector and divide the dataset<a id='Step_0' ></a>\n",
    "\n",
    " - Create the feature vector from the given columns.\n",
    " - When you create you feature vector, remember to exclude the column that you will be using for testing the accuracy of your model.\n",
    " - After creating a feature vector, split the dataset into two (e.g., training and testing) randomly and between 70 percent and 30 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate list of columns for creating feature vector for the datatype double`\n",
    "vectorlist = [column[0] for column in weather_df2.dtypes if column[1] == \"double\"]\n",
    "vectorlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Creating a feature vector using VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols=vectorlist[:-1], outputCol=\"features\")\n",
    "feature_vect = vector_assembler.transform(weather_df2)\n",
    "feature_vect.select('features').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train (70%) and test (30%)s\n",
    "(trainingData,testData) = feature_vect.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Apply Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 08: Apply machine learning classification algorithms on the dataset and compare their accuracy. Plot the accuracy as bar graph.<a id='Step_08' ></a>\n",
    "\n",
    " - Use `DecisionTreeClassifier()`, `RandomForestClassifier()`,`LogisticRegression()` & `GBTClassifier()` methods in spark to calculate the probability of the rain fall tomorrow based on the other related data points (e.g., temperature, wind, humidity).\n",
    " - Draw the graph (e.g. bar chart) to demonstrate the comparison of their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction of probability of rain fall tomorrow I had used four different machine learning algorithms to compare the accuracies in each algorithm.\n",
    "\n",
    "1. __`Decision Tree`__ : Decision Trees are a non-parametric supervised learning method used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. This is achieved in pyspark using function `DecisionTreeClassifier`.\n",
    "2. __`Random Forest`__ : Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.This is achieved in pyspark using function `RandomForestClassifier`.\n",
    "3. __`LogisticRegression`__ : Logistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).This is achieved in pyspark using function `LogisticRegression`. \n",
    "4. __`GBTClassifier`__ : Gradient-Boosted Trees (GBTs) are ensembles of decision trees. GBTs iteratively train decision trees in order to minimize a loss function. The spark.ml implementation supports GBTs for binary classification and for regression, using both continuous and categorical features. This is achieved in pyspark using function `GBTClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions of different Machine Learning algorithms\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to store predicted values of each algorithm\n",
    "prediction_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DecisionTreeClassifier to classify them.\n",
    "decision_tree = DecisionTreeClassifier(labelCol=\"RainTomorrowlabelIndex\", featuresCol=\"features\")\n",
    "# Fitting the above model on training data\n",
    "model = decision_tree.fit(trainingData)\n",
    "# Predicting the model on test data\n",
    "predictions = model.transform(testData)\n",
    "# Viewing the top 5 contents of predicted values on raintomorrow column\n",
    "predictions.select(\"prediction\", \"RainTomorrowlabelIndex\").show(5)\n",
    "# Evaluating the prediction column using MulticalssClassification evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"RainTomorrowlabelIndex\", \n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "accuracy_decisiontree = evaluator.evaluate(predictions)\n",
    "# Printing Error & Accuracy for Decision tree algorithm\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy_decisiontree))\n",
    "print(\"Test accuracy = %g \" % (accuracy_decisiontree*100),\"%\")\n",
    "# Storing the predicted value of Decision tree to dictionary\n",
    "prediction_dict['DecisionTreeClassifier'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RandomForest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RandomForestClassifier to classify them.\n",
    "random_forest = RandomForestClassifier(labelCol=\"RainTomorrowlabelIndex\", featuresCol=\"features\")\n",
    "# Fitting the above model on training data\n",
    "model = random_forest.fit(trainingData)\n",
    "# Predicting the model on test data\n",
    "predictions = model.transform(testData)\n",
    "# Viewing the top 5 contents of predicted values on raintomorrow column\n",
    "predictions.select(\"prediction\", \"RainTomorrowlabelIndex\").show(5)\n",
    "# Evaluating the prediction column using MulticalssClassification evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"RainTomorrowlabelIndex\", \n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "accuracy_randomforest = evaluator.evaluate(predictions)\n",
    "# Printing Error & Accuracy for Random Forest algorithm\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy_randomforest))\n",
    "print(\"Test accuracy = %g \" % (accuracy_randomforest*100),\"%\")\n",
    "# Storing the predicted value of Random Forest to dictionary\n",
    "prediction_dict['RandomForestClassifier'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LogisticRegression to classify them.\n",
    "logistic_regression = LogisticRegression(labelCol=\"RainTomorrowlabelIndex\", featuresCol=\"features\")\n",
    "# Fitting the above model on training data\n",
    "model = logistic_regression.fit(trainingData)\n",
    "# Predicting the model on test data\n",
    "predictions = model.transform(testData)\n",
    "# Viewing the top 5 contents of predicted values on raintomorrow column\n",
    "predictions.select(\"prediction\", \"RainTomorrowlabelIndex\").show(5)\n",
    "# Evaluating the prediction column using MulticalssClassification evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"RainTomorrowlabelIndex\", \n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "accuracy_logistic = evaluator.evaluate(predictions)\n",
    "# Printing Error & Accuracy for Logistic Regression algorithm\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy_logistic))\n",
    "print(\"Test accuracy = %g \" % (accuracy_logistic*100),\"%\")\n",
    "# Storing the predicted value of Logistic Regression to dictionary\n",
    "prediction_dict['LogisticRegression'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GBTClassifier to classify them.\n",
    "gbt_classifier = GBTClassifier(labelCol=\"RainTomorrowlabelIndex\", featuresCol=\"features\")\n",
    "# Fitting the above model on training data\n",
    "model = gbt_classifier.fit(trainingData)\n",
    "# Predicting the model on test data\n",
    "predictions = model.transform(testData)\n",
    "# Viewing the top 5 contents of predicted values on raintomorrow column\n",
    "predictions.select(\"prediction\", \"RainTomorrowlabelIndex\").show(5)\n",
    "# Evaluating the prediction column using MulticalssClassification evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"RainTomorrowlabelIndex\", \n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "accuracy_GBT = evaluator.evaluate(predictions)\n",
    "# Printing Error & Accuracy for GBTClassifier algorithm\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy_GBT))\n",
    "print(\"Test accuracy = %g \" % (accuracy_GBT*100),\"%\")\n",
    "# Storing the predicted value of GBTClassifier to dictionary\n",
    "prediction_dict['GBTClassifier'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __matplotlib__ : This library is used for plotting the contents of the extracted data.\n",
    " \n",
    " I had used matplotlib styling with the help of __`ggplot`__.\n",
    " \n",
    " Plotted the __`bar graph`__ by taking `accuracies` of each algorithms in `Y-axis` & `algorithms` in `X-axis`.\n",
    " \n",
    "__Understandings__:\n",
    " - Plotted the accuracy for different algorithms with the help of barchart.\n",
    " - For the algorithms `Decision Tree`,`Random Forest`,`Logistic Regression` accuracy values are almost similar whch is upto 83%.\n",
    " - Comparitively accuracy is bit higher for `GBT Classifier` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "accuracies=[accuracy_decisiontree*100,accuracy_randomforest*100,accuracy_logistic*100,accuracy_GBT*100]\n",
    "algorithms=['DecisionTreeClassifier','RandomForestClassifier','LogisticRegression','GBTClassifier']\n",
    "plt.figure(figsize=(15,10))\n",
    "bars = plt.bar(algorithms,accuracies,color = 'C1')#color=['red', 'green', 'blue', 'cyan'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Classification Algorithms',fontsize = 15)\n",
    "plt.ylabel('Accuracies for different algorithms',fontsize = 15)\n",
    "plt.title('Comparison of Accuracies for different Machine Learning algorithms ',fontsize = 15)\n",
    "# Visualising the values of each accuracy on bar chart for each algorithm\n",
    "for bar in bars:\n",
    "    value = bar.get_height()\n",
    "    plt.text(bar.get_x(),value, value,fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the contents of dictionary of predicted values for each algorithm\n",
    "prediction_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 09 : Calculate the confusion matrix and find the precision, recall, and F1 score of each classification algorithm. <a id='Step_09' ></a>\n",
    "\n",
    " - Calculate the Confusion matrix using confusionMatrix() method.\n",
    " - Find the Precision, recall and F1 score of each classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __`Confusion Matrix`__ : A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. Using `MulticlassMetrics` library and with the help of `ConfusionMatrix` function I had calculated the confusion matrix for each classification algorithm\n",
    "\n",
    "While there are many different types of classification algorithms, the evaluation of classification models all share similar principles. In a supervised classification problem, there exists a true output and a model-generated predicted output for each data point. For this reason, the results for each data point can be assigned to one of four categories:\n",
    "\n",
    " - `True Positive (TP)` - label is positive and prediction is also positive\n",
    " - `True Negative (TN)` - label is negative and prediction is also negative\n",
    " - `False Positive (FP)` - label is negative but prediction is positive\n",
    " - `False Negative (FN)` - label is positive but prediction is negative\n",
    "\n",
    "These four numbers are the building blocks for most classifier evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __`Precision`__ : Measures the percentage of the correct classification from the predicted members. Also called as `positive predictive value`. \n",
    "\n",
    "\\begin{gather*}\n",
    "    \\therefore Precision = \\frac{True Positive}{(True Positive + False Positive)}\n",
    "\\end{gather*}\n",
    "\n",
    "- __`Recall`__ : Measures the percentage of the correct classification from the overall members. Also called as `Sensitivity`. \n",
    "\n",
    "\\begin{gather*}\n",
    "    \\therefore Recall = \\frac{True Positive}{(True Positive + False Negative)}\n",
    "\\end{gather*}\n",
    "\n",
    "Both precision and recall are therefore based on an understanding and measure of relevance. \n",
    "\n",
    "- __`F1 Score`__ : Measures the balances of Precision & Recall. Also called as `F-score or F-measure`. \n",
    "\n",
    "\\begin{gather*}\n",
    "    \\therefore F1Score = \\frac{2*(Precision * Recall)}{(Precision + Recall)}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing MulticlassMetrics library for calculating Confusion Matrix\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using for loop storing the predicted values for each algorithm\n",
    "for key,value in prediction_dict.items():\n",
    "    predict = value.select(\"prediction\", \"RainTomorrowlabelIndex\")\n",
    "# Converting the predicted values to RDD\n",
    "    rdd_predict = predict.rdd\n",
    "# Created RDD is passed through MulticlassMetrics function\n",
    "    confusion_matrix = MulticlassMetrics(rdd_predict)\n",
    "# Calculating Confusion matrix for all algorithms\n",
    "    ConfusionMatrix = confusion_matrix.confusionMatrix().toArray()\n",
    "# Declaring True positive, True Negative,False positive, False Negative for predicted column\n",
    "    tp = value[(value.RainTomorrowlabelIndex == 0) & (value.prediction == 0)].count()\n",
    "    tn = value[(value.RainTomorrowlabelIndex == 1) & (value.prediction == 1)].count()\n",
    "    fp = value[(value.RainTomorrowlabelIndex == 1) & (value.prediction == 0)].count()\n",
    "    fn = value[(value.RainTomorrowlabelIndex == 0) & (value.prediction == 1)].count()\n",
    "# Calculating Precision for all algorithms\n",
    "    Precision = tp/(tp+fp)\n",
    "# Calculating Recall for all algorithms\n",
    "    Recall = tp/(tp+fn)\n",
    "# Calculating F1 Score for all algorithms\n",
    "    F1Score = 2*(Precision * Recall) / (Precision + Recall)\n",
    "    print('Classification Algorithm:',key)\n",
    "    print('Confusion Matrix:\\n',ConfusionMatrix)\n",
    "    print('Precision:\\n',Precision)\n",
    "    print('Recall:\\n',Recall)\n",
    "    print('F1 Score:\\n',F1Score)\n",
    "    print('-'*20 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain how the accuracy of the predication can be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above models we can infer that accuracy of `GBT Classifier` algorithm has little higher accuracy compared to other three algorithms. More the accuracy, will leads to better prediction.\n",
    "\n",
    "Also there are few other ways which we can consider to improve the accuracy of the prediction:\n",
    "\n",
    "1. In order to improve accuracy we Add more parameters i.e, this can be done by adding `max-depth` parameter in `Decision-Tree` algorithm, changing the number of trees in `Random Forest`, adding `max-iters` parameter in `Logistic Regression` & `GBT Classifier` algorithm.\n",
    "\n",
    "2. To make our prediction more accurate, we can `change the random sampling rate of training & test data` till we achieve more accuracy i.e, we can perform `cross-validation` which means try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.\n",
    "\n",
    "3. One more way of achieving better accuracy is `Ensemble Method` which combines the results of many weak Machine Learning models and give better results. Best method is by `Bagging` also called as `Bootstrap Aggregating`. Even though this method is more complex than other usual methods, it will yield better results in improving accuracy.\n",
    "\n",
    "4. Other way is to treat both `Missing Values or Null Values` & `Outlier values` in an effecive manner. Here for numerical columns I had imputed with mean of its values. Instead using prediction models like `KNN` or `Linear Regression` we can predict the missing values which yields better accuracy than mean imputation. Also, removing outliers initially before imputing will yield better accuracy.\n",
    "\n",
    "These are the few ways where in general we can improve the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assessment analyze rain data in Australia. Different machine learning algorithms can be used to model and predict rainfall in the Python using Apache Pyspark. The main outcomes achieved while applying these techniques were:\n",
    "\n",
    "- __Import pyspark and initialize Spark__\n",
    "- __Load the dataset and print the schema and total number of entries__\n",
    "- __Delete columns from the dataset__\n",
    "- __Print the number of missing data in each column__\n",
    "- __Fill the missing data with average value and maximum occurrence value__\n",
    "- __Data transformation__\n",
    "- __Create the feature vector and divide the dataset__\n",
    "- __Apply machine learning classification algorithms on the dataset and compare their accuracy. Plot the accuracy as bar graph__\n",
    "- __Calculate the confusion matrix and find the precision, recall, and F1 score of each classification algorithm__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Matplotlib Official Documentation)](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/\n",
    "* https://stackoverflow.com/questions/41032256/get-same-value-for-precision-recall-and-f-score-in-apache-spark-logistic-regres\n",
    "* [Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
